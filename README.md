# Adversarial Machine Learning for Anti-Malware Software

presented in Secuinside 2017  @nababora


# 소개
제가 발표한 내용은 Adversarial Machine Learning 이라는 주제로,
말 그대로 머신러닝 모델을 공격하는 방법과 사례를 소개 했습니다.
사실 원래 목적은 머신러닝 기반 모델도 설계 단계부터 adversary를 고려해야 한다는 것을 증명하는 것이었습니다.
(특히 보안 분야에 ML을 도입하려면 반드시 고려해야 하는 부분입니다)

그리하여, 제가 만든 머신러닝 기반 악성코드 탐지 모델을 공격하는 adversarial 모델을 만들었습니다.
하지만 여기서 '공격'이라 함은 단순히 악성코드를 정상 파일처럼 보이게 하는 것 뿐입니다(변장).

저도 발표자료에서 소개한 wannacry 샘플에서도 볼 수 있듯이
이 단순한 공격이 실제로 많은 벤더사의 제품의 탐지를 우회할 수 있습니다.
처음엔 저도 의아했지만.
사실 생각해보면 머신러닝 기반이라고 하더라도 기존 시그니처 방식에서 사용하는 악성코드의 '특징'들을 사용할 수 밖에 없습니다.
판단하는 기준만 다를 뿐이죠. 기존 방식은 알고리즘으로 분류한다면, 머신러닝은 '확률'을 기반으로 한다는 차이가 있겠죠.
즉, '머신러닝 기반을 타겟으로 하는 공격 코드를 기존 시그니처 방식에도 적용 가능하다' 라는 결론이 나옵니다. 
제가 우회를 성공한 백신들이 취약하다? 우회 안 된 제품은 안전하다? 이런 잣대로 판단해선 안 됩니다.
그냥 제가 공격(변장)에 사용한 특징들과 겹치는 부분이 많은 제품들은 우회가 된 것이고, 아닌 제품들은 우회가 안 된 것 뿐입니다.

물론, 제 공격 코드는 공개할 계획이 없습니다. 
하지만 여러분. 올해 블랙햇에서 관련 내용이 발표될 예정입니다. 
블랙햇 발표는 코드 공개가 원칙인거 다들 알고 계시죠?
제가 생각하는 바가 맞다면. 제가 만든 공격 모델보다 더 정교하고 자동화된 '변장'(악성을 정상처럼)을 할 수 있습니다.
제가 원래 계획했던 기능들을 다 담았다면, 메타스플로잇 인코더 돌리듯이 스크립트 하나로 간단하게 백신들을 다 우회할 수 있을 겁니다.
파장이 크지 않을까요?

그렇다면 이러한 공격들에 어떻게 대응해야 할까요?

기존 시그니처 방식만 쓰는 제품은 사실 저도 잘 모르겠습니다.. 제 발표에서 소개한 우회 샘플 패턴으로 막아도 소용이 없을 것 같습니다.
대단한 우회 기법을 적용한 것이 아니라, 단순히 악성코드를 process explorer 처럼 보이도록 만든 것 뿐입니다. 저 패턴이 막히면
그냥 정상으로 가장할 다른 프로그램을 가져다가 변장하면 됩니다. 

머신 러닝 모델을 쓰고 계시다면 지금이라도 준비를 하셔야 됩니다.
사실 기존에 나온 자료들을 보면 'robust modeling'을 해야 한다고 나오는데 
저도 이쪽 분야의 전문가는 아니지만 제가 아는 선에서 정리해 보면 크게 세 가지 정도가 될 수 있겠네요.

첫째, 쉽게 흉내낼 수 없는 '특징'을 사용해야 합니다.
 - 바이너리에서 단편적으로 추출한 특징을 사용하면 adversarial 공격에 취약할 수 있습니다. 
 사실 이 부분은 정답이 정해져 있는 것은 아니지만 정적 정보만 가지고 특징을 만든다고 하더라도 단순히 dos header, optional header 등
 헤더 값들을 파싱하고 그 특징을 그대로 쓰는 것이 아니라 추출한 정보를 한 번 더 가공해서 새로운 의미를 가진 특징으로 만들어 주는 방법을 사용해야 합니다.

둘째, 주성분 분석(PCA)을 통해 기존 특징 벡터를 변환 합니다.
 - 이 부분은 저도 실험을 해본 것은 아니지만, 주성분 분석을 통해 특징 축 전체를 변환하게 되면 기존 특징 벡터에서 가장 핵심이 되는 특징 축을 중심으로 
 공간이 재정렬 되는 효과를 얻을 수 있습니다. 이를 통해 단순히 단순히 몇 개의 특징을 변장하는 공격에 탐지 모델을 조금이나마 둔감하게 만들 수 있을 것 같습니다. 

셋째, adversarial 모델을 구축해 모델링 단계에 적용합니다.
 - 이건 좀 스케일이 큰 부분이라 따로 언급하지 않겠습니다. 제가 발표에서 소개한 모델을 직접 구현해야 합니다 ^^;

아직 해당 분야는 이론적으로는 증명이 되었지만 실제 적용 사례는 많지 않은 상태입니다.
초기 모델링 단계부터 adversarial 모델을 적용해서 더 안전한 백신을 개발할 수 있었으면 좋겠습니다.